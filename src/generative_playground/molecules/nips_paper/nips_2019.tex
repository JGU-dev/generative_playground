\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Batch-Advatage Transformer with Hypergraph Optimized Grammar (BAT/HOG)}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Egor Kraev \\
  Mosaic Smart Data Ltd \\
  \texttt{egor.kraev@gmail.com} \\
  \And
  Mark Harley \\
  Mosaic Smart Data Ltd \\
  \texttt{mharley.code@gmail.com}
}

\begin{document}

\maketitle

\begin{abstract}
We present a novel approach to the issue of molecular optimization. Our approach uses a hypergraph replacement grammar inferred from the ZINC database, with grammar construction optimized for molecular structure creation. We treat the optimization as a reinforcement learning problem, using a batch-advantage modification of the policy gradient algorithm - using individual rewards minus the batch average reward to weight the log probability loss. The reinforcement learning agent is tasked with building molecules using this grammar, with the goal of maximizing benchmark scores available from the literature. To do so, the agent has policies both to choose the next node in the graph to expand and to select the next grammar rule to apply. The policies are implemented using the Transformer architecture with the partially expanded graph as the input. We achieve state of the art performance on common benchmarks from the literature, such as penalized logP and QED, with only hundreds of steps (without pre-training) on a budget GPU instance. Competitive performance is obtained on more advanced GuacaMol v2 goal-oriented benchmarks. Coupled with a Transformer based discriminator, the model achieves competitive results on the GuacaMol distribution benchmarks; training is stable over a range of hyperparameter values.
\end{abstract}

\section{Introduction}

\section{Hypergraph grammar}\label{sec:valid_smiles}
Will need some pictures here like in Kajino, to explain how the whole thing works!
\subsection{Definition}\label{sec:cfgs}
\subsection{Parsing algorithm}\label{sec:valences}
\subsubsection{Extraction of Hypergraph Cliques}\label{sec:cliques}
\subsection{Rule-pair compression}
\subsection{Using the grammar to create new molecules}
\subsunsection{Conditional and unconditional rule frequencies}\label{sec:hydrogen}
\subsubsection{Making sure the expansions terminate}\label{sec:termination}
\subsection{Grammar conciseness and expressiveness}\label{sec:expressiveness}


\section{Model choice}\label{sec:model}

\subsection{Reinforcement learning}\label{sec:rl}
\subsubsection{Batch Advantage}
\subsubsection{Record rewards}

\subsection{Architecture}\label{sec:architecture}

\subsection{Training}\label{sec:training}

\subsection{Optimization and the reward function}\label{sec:reward}


\section{Results}\label{sec:results}

\subsection{GuacaMol Benchmarks}

\subsection{Ablation Studies}


\subsubsection*{Acknowledgments}

\section*{References}
\medskip

\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
