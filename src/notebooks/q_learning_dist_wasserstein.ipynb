{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_position(max_height, max_width):\n",
    "    return np.random.randint(0, max_height), np.random.randint(0, max_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x, y):\n",
    "    return np.array_equal(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    def __init__(self, height, width):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        self.initial_state = None\n",
    "        self.initial_player_pos = None\n",
    "        self.state = np.zeros((height, width, 4), dtype=int)\n",
    "        self.player = np.array([0, 0, 0, 1])\n",
    "        self.wall = np.array([0, 0, 1, 0])\n",
    "        self.pit = np.array([0, 1, 0, 0])\n",
    "        self.goal = np.array([1, 0, 0, 0])\n",
    "        self.player_position = None\n",
    "        self.world_ = None\n",
    "        \n",
    "        self.up = 0\n",
    "        self.right = 1\n",
    "        self.down = 2\n",
    "        self.left = 3\n",
    "        \n",
    "    def set_player_position(self, position):\n",
    "        if self.player_position is not None:\n",
    "            self.state[tuple(self.player_position)] = np.zeros(4)\n",
    "        self.player_position = tuple(position)\n",
    "        self.state[tuple(position)] = self.player\n",
    "        \n",
    "    def save_state(self):\n",
    "        self.initial_state = np.copy(self.state)\n",
    "        self.initial_player_pos = np.copy(self.player_position)\n",
    "        \n",
    "    @classmethod\n",
    "    def deterministic_easy(cls, height=4, width=4):\n",
    "        world = cls(height, width)\n",
    "        #place player\n",
    "        world.set_player_position((0, 1))\n",
    "        #place goal\n",
    "        world.state[3, 3] = world.goal\n",
    "        world.save_state()\n",
    "        return world\n",
    "\n",
    "    @classmethod\n",
    "    def deterministic(cls, height=4, width=4):\n",
    "        world = cls(height, width)\n",
    "        #place player\n",
    "        world.set_player_position((0, 1))\n",
    "        #place wall\n",
    "        world.state[2, 2] = world.wall\n",
    "        #place pit\n",
    "        world.state[1, 1] = world.pit\n",
    "        #place goal\n",
    "        world.state[3, 3] = world.goal\n",
    "        world.save_state()\n",
    "        return world\n",
    "    \n",
    "    @classmethod\n",
    "    def random_player_pos(cls, height=4, width=4):\n",
    "        world = cls(height, width)\n",
    "        pos = random_position(height, width)\n",
    "        while pos in ((2, 2), (1, 1), (1, 2)):\n",
    "            pos = random_position(height, width)\n",
    "        #place player\n",
    "        world.set_player_position(pos)\n",
    "        #place wall\n",
    "        world.state[2, 2] = world.wall\n",
    "        #place pit\n",
    "        world.state[1, 1] = world.pit\n",
    "        #place goal\n",
    "        world.state[1, 2] = world.goal\n",
    "        world.save_state()\n",
    "        return world\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, height=4, width=4):\n",
    "        world = cls(height, width)\n",
    "        length = height * width\n",
    "        places = [(i, j) for i in range(height) for j in range(width)]\n",
    "        positions = random.sample(places, 4)\n",
    "        #place player\n",
    "        world.set_player_position(positions[0])\n",
    "        #place wall\n",
    "        world.state[positions[1]] = world.wall\n",
    "        #place pit\n",
    "        world.state[positions[2]] = world.pit\n",
    "        #place goal\n",
    "        world.state[positions[3]] = world.goal\n",
    "        world.save_state()\n",
    "        return world\n",
    "    \n",
    "    def step(self, action):\n",
    "        diff = (0, 0)\n",
    "        if action == self.up and self.player_position[0] > 0:\n",
    "            diff = (-1, 0)\n",
    "        elif action == self.right and self.player_position[1] < self.width - 1:\n",
    "            diff = (0, 1)\n",
    "        elif action == self.down and self.player_position[0] < self.height - 1:\n",
    "            diff = (1, 0)\n",
    "        elif action == self.left and  self.player_position[1] > 0:\n",
    "            diff = (0, -1)\n",
    "        \n",
    "        old_pos = tuple(np.copy(self.player_position))\n",
    "        new_pos = tuple(np.add(self.player_position, diff))\n",
    "        done = False\n",
    "        reward = -1\n",
    "        if compare(self.state[new_pos], self.wall):\n",
    "            new_pos = old_pos\n",
    "        elif compare(self.state[new_pos], self.pit):\n",
    "            done = True\n",
    "            reward = -10\n",
    "        elif compare(self.state[new_pos], self.goal):\n",
    "            done = True\n",
    "            reward = 10\n",
    "        old_state = np.copy(self.state)\n",
    "        self.set_player_position(new_pos)\n",
    "        return old_state, reward, self.state, done\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.initial_state)\n",
    "        self.player_position = np.copy(self.initial_player_pos)\n",
    "        return self.state\n",
    "    \n",
    "    def display(self):\n",
    "        grid = np.empty((self.height, self.width), dtype=str)\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                point = self.state[i, j]\n",
    "                if compare(point, self.player):\n",
    "                    grid[i, j] = '@'\n",
    "                elif compare(point, self.wall):\n",
    "                    grid[i, j] = 'W'\n",
    "                elif compare(point, self.goal):\n",
    "                    grid[i, j] = '+'\n",
    "                elif compare(point, self.pit):\n",
    "                    grid[i, j] = '^'\n",
    "                else:\n",
    "                    grid[i, j] = ' '\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDistFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(64, 164)\n",
    "        self.l2 = nn.Linear(164, 150)\n",
    "        self.l3 = nn.Linear(150, 21 * 4)\n",
    "        self.smax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        # view -> softmax\n",
    "        x = torch.cat(\n",
    "            [F.softmax(x[21 * a:21 * (a + 1)]) for a in range(4)],\n",
    "            dim=0\n",
    "        )\n",
    "        return x\n",
    "    \n",
    "    def fit_step(\n",
    "        self,\n",
    "        old_state,\n",
    "        new_state,\n",
    "        action,\n",
    "        reward,\n",
    "        range_,\n",
    "        gamma,\n",
    "        loss_fun,\n",
    "        optimizer,\n",
    "        grad_clip: Optional[float] = None,\n",
    ") -> None:\n",
    "        self.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = self(old_state)\n",
    "        \n",
    "        new_qdist = self(new_state)\n",
    "        len_range = len(range_)\n",
    "        # Softmax this rather than using max EV\n",
    "        evs = [\n",
    "            sum(z * p for z, p in zip(range_, new_qdist[len_range * a:len_range * (a + 1)]))\n",
    "            for a in range(4)\n",
    "        ]\n",
    "        a = np.argmax(evs)\n",
    "        dist = new_qdist[21 * a:21 * (a + 1)]\n",
    "        m = torch.zeros((21,))\n",
    "        for i, z in enumerate(range_):\n",
    "            tzj = reward + gamma * z\n",
    "            if tzj < -10:\n",
    "                tzj = -10\n",
    "            elif tzj > 10:\n",
    "                tzj = 10\n",
    "            bj = tzj + 10\n",
    "            l = int(np.floor(bj))\n",
    "            u = int(np.ceil(bj))\n",
    "            m[l] += dist[i] * (u - bj)\n",
    "            m[u] += dist[i] * (bj - l)\n",
    "        \n",
    "        loss = loss_fun(outputs[21 * action:21 * (action + 1)], m.detach())\n",
    "        loss.backward()\n",
    "        if grad_clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.parameters(),\n",
    "                grad_clip\n",
    "            )\n",
    "        optimizer.step()\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, epsilon, gamma, lr=0.01, max_steps=100):\n",
    "        self.q = QDistFunction()\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.max_steps = max_steps\n",
    "        self.loss_fun = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.q.parameters(), lr)\n",
    "        self.range_ = list(range(-10, 11))\n",
    "        \n",
    "    def run_episode(self):\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        while not done:\n",
    "            qdist = self.q(\n",
    "                torch.tensor(self.env.state, dtype=torch.float32)\n",
    "            )\n",
    "            samples = torch.cat(\n",
    "                [torch.multinomial(qdist[21 * a:21 * (a + 1)], 1)\n",
    "                for a in range(4)]\n",
    "            )\n",
    "            action = samples.max(0)[1]\n",
    "                \n",
    "            old_state, reward, _, done = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            self.q.fit_step(\n",
    "                torch.tensor(old_state, dtype=torch.float32),\n",
    "                torch.tensor(self.env.state, dtype=torch.float32),\n",
    "                action,\n",
    "                reward,\n",
    "                self.range_,\n",
    "                self.gamma,\n",
    "                self.loss_fun,\n",
    "                self.optimizer\n",
    "            )\n",
    "        return total_reward\n",
    "    \n",
    "    def run_model(self, world=0):\n",
    "        done = False\n",
    "        self.env.reset()\n",
    "        print(self.env.display())\n",
    "        for _ in range(self.max_steps):\n",
    "            qdist = self.q(torch.tensor(self.env.state, dtype=torch.float32))\n",
    "            len_range = len(self.range_)\n",
    "            evs = [\n",
    "                sum(z * p for z, p in zip(self.range_, qdist[len_range * a:len_range * (a + 1)]))\n",
    "                for a in range(4)\n",
    "            ]\n",
    "            action = np.argmax(evs)\n",
    "\n",
    "            _, _, _, done = self.env.step(action)\n",
    "            print(self.env.display())\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    def train(self, epochs=1000):\n",
    "        self.env.reset()\n",
    "        rewards = np.zeros(epochs)\n",
    "        for i in tqdm(range(epochs)):\n",
    "            rewards[i] = self.run_episode()\n",
    "            self.env.reset()\n",
    "        return pd.Series(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    Gridworld.deterministic_easy(),\n",
    "    epsilon=0.1,\n",
    "    gamma=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = agent.train(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.expanding().mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
