{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler\n",
    "import torch\n",
    "print(torch.cuda.is_available(), torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-3.3854e-18\n",
      " 4.5765e-41\n",
      "-3.3854e-18\n",
      " 4.5765e-41\n",
      " 4.4842e-44\n",
      "[torch.FloatTensor of size 5x1]\n",
      "\n",
      "\n",
      "1.00000e-18 *\n",
      " -3.3854\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e-41 *\n",
      "  4.5765\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e-18 *\n",
      " -3.3854\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e-41 *\n",
      "  4.5765\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e-44 *\n",
      "  4.4842\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor(5,1)\n",
    "print(a)\n",
    "for a_ in a:\n",
    "    print(a_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the function generating warp matrix from vector of numbers between 0 and 1\n",
    "# reconcile new version against old\n",
    "from reshape import WarpMatrix,WarpMatrixOld\n",
    "from torch.autograd import Variable\n",
    "from utils import to_gpu\n",
    "import torch\n",
    "\n",
    "w1=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "w2=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "w2.data = w1.data\n",
    "wmat1 = WarpMatrix.apply(w1)\n",
    "loss1 = wmat1.sum(0)[1]\n",
    "loss1.backward()\n",
    "wmat2 = WarpMatrixOld.apply(w2)\n",
    "loss2 = wmat2.sum(0)[1]\n",
    "loss2.backward()\n",
    "\n",
    "print((wmat1-wmat2).abs().max(), \n",
    "      (loss1-loss2).abs().max(),\n",
    "      (w1.grad-w2.grad).abs().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.6059  0.0524  0.0155  0.2362  0.0900\n",
      "  0.0000  0.0000  0.0000  0.0000  0.8605\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.4488  0.5512  0.0000  0.0000  0.0000\n",
      "  0.0000  0.3491  0.2183  0.1515  0.2319\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "[torch.cuda.FloatTensor of size 2x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now try feeding it batch data\n",
    "w1=Variable(to_gpu(torch.FloatTensor(2,5,1).uniform_()), requires_grad = True)\n",
    "wmat1 = WarpMatrix.apply(w1)\n",
    "loss1 = wmat1.sum(0).sum(0)[1]\n",
    "loss1.backward()\n",
    "print(wmat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# profile WarpMatrix autograd\n",
    "# class dummy:\n",
    "#     pass\n",
    "\n",
    "# ctx = dummy()\n",
    "# w=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "# wmat = WarpMatrix.forward(ctx,w.data)\n",
    "# %lprun -f WarpMatrix.backward WarpMatrix.backward(ctx,Variable(wmat))\n",
    "#print(wmat,pseudo_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's test the whole fitted-weights-compression transform as a layer\n",
    "from reshape import FittedWarp\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "true_w=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "warp = FittedWarp(true_w.shape)\n",
    "x = Variable(to_gpu(torch.randn(2,10,warp.input_shape[1])))\n",
    "out = warp(x)\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "#print(out, loss,warp.w.grad)\n",
    "# a =w.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from visualize import make_dot\n",
    "# make_dot(loss)\n",
    "# wait for feedback from the function author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the 'true' model\n",
    "true_w=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "true_warp = to_gpu(FittedWarp(true_w.shape))\n",
    "true_warp.w.data = true_w.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 5]) 3\n"
     ]
    }
   ],
   "source": [
    "# testing a pytorch-style data loader\n",
    "from data_sources import DatasetFromModel\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = DatasetFromModel(100, 100, true_warp)\n",
    "dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=True)\n",
    "out = next(dataloader.__iter__())\n",
    "print(out[0].shape,len(out[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "0.12665530607104303 100\n",
      "we're improving! 0.11261790245771408\n",
      "successfully saved model\n",
      "epoch  1\n",
      "0.11269098564982415 100\n",
      "we're improving! 0.10550092160701752\n",
      "successfully saved model\n",
      "epoch  2\n",
      "0.10176034390926361 100\n",
      "we're improving! 0.09657362848520279\n",
      "successfully saved model\n",
      "epoch  3\n",
      "0.09398640148341655 100\n",
      "we're improving! 0.08809714764356613\n",
      "successfully saved model\n",
      "epoch  4\n",
      "0.09119180560112 100\n",
      "we're improving! 0.08615005761384964\n",
      "successfully saved model\n",
      "epoch  5\n",
      "0.08753484457731248 100\n",
      "we're improving! 0.08602458983659744\n",
      "successfully saved model\n",
      "epoch  6\n",
      "0.08639505572617054 100\n",
      "we're improving! 0.0832962691783905\n",
      "successfully saved model\n",
      "epoch  7\n",
      "0.08417974315583705 100\n",
      "we're improving! 0.08107614517211914\n",
      "successfully saved model\n",
      "epoch  8\n",
      "0.08301712039858103 100\n",
      "epoch  9\n",
      "0.08189088560640811 100\n",
      "epoch  10\n",
      "0.08147070843726396 100\n",
      "we're improving! 0.07997623085975647\n",
      "successfully saved model\n",
      "epoch  11\n",
      "0.08237850807607174 100\n",
      "epoch  12\n",
      "0.07981310684233904 100\n",
      "epoch  13\n",
      "0.07907096546143294 100\n",
      "epoch  14\n",
      "0.07974742811173201 100\n",
      "we're improving! 0.07774578034877777\n",
      "successfully saved model\n",
      "epoch  15\n",
      "0.07904970571398735 100\n",
      "epoch  16\n",
      "0.07769114404916763 100\n",
      "epoch  17\n",
      "0.08008796609938144 100\n",
      "epoch  18\n",
      "0.07959166042506695 100\n",
      "epoch  19\n",
      "0.07782440029084682 100\n",
      "epoch  20\n",
      "0.07956088248640299 100\n",
      "epoch  21\n",
      "0.07889120183885097 100\n",
      "epoch  22\n",
      "0.08019298076629638 100\n",
      "epoch  23\n",
      "0.0782470330223441 100\n",
      "epoch  24\n",
      "0.07945169381797314 100\n",
      "epoch  25\n",
      "0.08032119397073983 100\n",
      "epoch  26\n",
      "0.08030086949467659 100\n",
      "epoch  27\n",
      "0.07873783402144909 100\n",
      "epoch  28\n",
      "0.07934473466128111 100\n",
      "epoch  29\n",
      "0.0802236258238554 100\n",
      "epoch  30\n",
      "0.07971183001995087 100\n",
      "epoch  31\n",
      "0.07894597485661507 100\n",
      "epoch  32\n",
      "0.07922378193587065 100\n",
      "epoch  33\n",
      "0.07997051775455474 100\n",
      "we're improving! 0.07628455013036728\n",
      "successfully saved model\n",
      "epoch  34\n",
      "0.07962985754013062 100\n",
      "epoch  35\n",
      "0.07830389630049467 100\n",
      "epoch  36\n",
      "0.07814356118440628 100\n",
      "epoch  37\n",
      "0.07807766664773226 100\n",
      "epoch  38\n",
      "0.07961795773357153 100\n",
      "epoch  39\n",
      "0.07944682516157626 100\n",
      "epoch  40\n",
      "0.07864184886217117 100\n",
      "epoch  41\n",
      "0.0798394676297903 100\n",
      "epoch  42\n",
      "0.07928879342973233 100\n",
      "epoch  43\n",
      "0.07854839954525232 100\n",
      "epoch  44\n",
      "0.0798713843896985 100\n",
      "we're improving! 0.07559223473072052\n",
      "successfully saved model\n",
      "epoch  45\n",
      "0.07956553630530834 100\n",
      "epoch  46\n",
      "0.0799200838059187 100\n",
      "epoch  47\n",
      "0.07814266838133335 100\n",
      "epoch  48\n",
      "0.07927308842539788 100\n",
      "epoch  49\n",
      "0.07830695316195488 100\n",
      "epoch  50\n",
      "0.0779699794948101 100\n",
      "epoch  51\n",
      "0.0787089478969574 100\n",
      "epoch  52\n",
      "0.07839837729930878 100\n",
      "epoch  53\n",
      "0.07827844027429819 100\n",
      "epoch  54\n",
      "0.07942823544144631 100\n",
      "epoch  55\n",
      "0.07756194971501827 100\n",
      "epoch  56\n",
      "0.07701864197850228 100\n",
      "epoch  57\n",
      "0.07884573947638274 100\n",
      "epoch  58\n",
      "0.07870278831571341 100\n",
      "epoch  59\n",
      "0.07759139325469733 100\n",
      "epoch  60\n",
      "0.07743779551237821 100\n",
      "epoch  61\n",
      "0.08070931695401669 100\n",
      "epoch  62\n",
      "0.07837682042270899 100\n",
      "epoch  63\n",
      "0.07878473620861769 100\n",
      "epoch  64\n",
      "0.0773933356627822 100\n",
      "epoch  65\n",
      "0.0803947114944458 100\n",
      "epoch  66\n",
      "0.07989969667047263 100\n",
      "epoch  67\n",
      "0.08050603963434697 100\n",
      "epoch  68\n",
      "0.07835481882095337 100\n",
      "epoch  69\n",
      "0.07743471328169108 100\n",
      "epoch  70\n",
      "0.07836879197508097 100\n",
      "epoch  71\n",
      "0.080904247649014 100\n",
      "epoch  72\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-606b518c1261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         save_path=save_path)\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mmy_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;31m#%lprun -f fit my_fit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#print(model.w, true_warp.w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-606b518c1261>\u001b[0m in \u001b[0;36mmy_fit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         save_path=save_path)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmy_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/basic_pytorch/fit.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train_gen, valid_gen, model, optimizer, scheduler, epochs, criterion, save_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mloss_\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now let's fit those weights\n",
    "from torch.optim import lr_scheduler\n",
    "from data_sources import data_gen\n",
    "from torch.autograd import Variable\n",
    "from fit import fit\n",
    "from models import FittedWarpWithConvolution, FittedWarp\n",
    "from utils import to_gpu\n",
    "\n",
    "model_type = FittedWarpWithConvolution\n",
    "\n",
    "# 'true' model\n",
    "true_w=Variable(to_gpu(torch.randn(100,1)), requires_grad = True)\n",
    "true_warp = to_gpu(model_type(w=true_w))\n",
    "\n",
    "\n",
    "# datasets generated from 'true' model\n",
    "train_dataset = DatasetFromModel(100, 100, true_warp)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1,\n",
    "                        shuffle=True)\n",
    "\n",
    "valid_dataset = DatasetFromModel(1000, 2, true_warp)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2,\n",
    "                        shuffle=True)\n",
    "\n",
    "# create a model with random weights, for training\n",
    "#model = to_gpu(model_type(true_w.shape))\n",
    "# and all the other bits we need\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "epochs = 1000\n",
    "save_path = 'test.mdl'\n",
    "\n",
    "\n",
    "def my_fit():\n",
    "    fit(train_gen = train_dataloader,\n",
    "        valid_gen = valid_dataloader,\n",
    "        model = model,\n",
    "        optimizer = optimizer,\n",
    "        scheduler = scheduler,\n",
    "        epochs = epochs,\n",
    "        criterion = criterion,\n",
    "        save_path=save_path)\n",
    "\n",
    "my_fit()\n",
    "#%lprun -f fit my_fit()\n",
    "#print(model.w, true_warp.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2d63d091528e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# approximations and returns True if they all verify this condition.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarpMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m \u001b[0mdifferences\u001b[0m \u001b[0msatisfy\u001b[0m \u001b[0mallclose\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \"\"\"\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_differentiable_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfail_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradchek takes a tuple of tensor as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(30,20).double(), requires_grad=True),)\n",
    "test = gradcheck(WarpMatrix.apply, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "class MyFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        output = torch.sign(input)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # saved tensors - tuple of tensors, so we need get first\n",
    "        input, = ctx.saved_variables\n",
    "#         grad_output[input.ge(1)] = 0\n",
    "#         grad_output[input.le(-1)] = 0\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "# usage\n",
    "x = torch.autograd.Variable(torch.randn(10, 20), requires_grad = True)\n",
    "y = MyFunction.apply(x)\n",
    "# or\n",
    "# my_func = MyFunction.apply\n",
    "# y = my_func(x)\n",
    "loss = y.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "x = Variable(torch.randn(10, 20), requires_grad=False)\n",
    "y = Variable(torch.randn(10, 3), requires_grad=False)\n",
    "# define some weights\n",
    "w1 = Variable(torch.randn(20, 5), requires_grad=True)\n",
    "w2 = Variable(torch.randn(5, 3), requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "for step in range(5):\n",
    "    pred = F.sigmoid(x @ w1)\n",
    "    pred = F.sigmoid(pred @ w2)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # manually zero all previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    # calculate new gradients\n",
    "    loss.backward()\n",
    "    # apply new gradients\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "first_counter = torch.Tensor([0])\n",
    "second_counter = torch.Tensor([10])\n",
    "some_value = torch.Tensor(15)\n",
    "\n",
    "while (first_counter < second_counter)[0]:\n",
    "    first_counter += 2\n",
    "    second_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next([1].iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Example of using Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 20, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(20, 64, 5),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# Example of using Sequential with OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "    ('relu2', nn.ReLU())\n",
    "]))\n",
    "\n",
    "#output = model(some_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, kernel_size=3, padding=1, stride=1),\n",
    "            nn.Conv2d(12, 24, kernel_size=3, padding=1, stride=1),\n",
    "        )\n",
    "        self.second_extractor = nn.Conv2d(\n",
    "            24, 36, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.second_extractor(x)\n",
    "        # note that we may call same layer twice or mode\n",
    "        x = self.second_extractor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        output = torch.sign(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # saved tensors - tuple of tensors, so we need get first\n",
    "        input, = ctx.saved_variables\n",
    "        grad_output[input.ge(1)] = 0\n",
    "        grad_output[input.le(-1)] = 0\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "# usage\n",
    "x = torch.randn(10, 20)\n",
    "y = MyFunction.apply(x)\n",
    "# or\n",
    "my_func = MyFunction.apply\n",
    "print(MyFunction)\n",
    "y = my_func(x)\n",
    "\n",
    "\n",
    "# and if we want to use inside nn.Module\n",
    "class MyFunctionModule(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MyFunction.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "### tensor example\n",
    "x_cpu = torch.randn(10, 20)\n",
    "w_cpu = torch.randn(20, 10)\n",
    "# direct transfer to the GPU\n",
    "x_gpu = x_cpu.cuda()\n",
    "w_gpu = w_cpu.cuda()\n",
    "result_gpu = x_gpu @ w_gpu\n",
    "# get back from GPU to CPU\n",
    "result_cpu = result_gpu.cpu()\n",
    "\n",
    "### model example\n",
    "model = model.cuda()\n",
    "# train step\n",
    "inputs = Variable(inputs.cuda())\n",
    "outputs = model(inputs)\n",
    "# get back from GPU to CPU\n",
    "outputs = outputs.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# check is cuda enabled\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# set required device\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# work with some required cuda device\n",
    "with torch.cuda.device(1):\n",
    "    # allocates a tensor on GPU 1\n",
    "    a = torch.cuda.FloatTensor(1)\n",
    "    assert a.get_device() == 1\n",
    "\n",
    "    # but you still can manually assign tensor to required device\n",
    "    d = torch.randn(2).cuda(2)\n",
    "    assert d.get_device() == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# new way with `init` module\n",
    "w = torch.Tensor(3, 5)\n",
    "torch.nn.init.normal(w)\n",
    "# work for Variables also\n",
    "w2 = Variable(w)\n",
    "torch.nn.init.normal(w2)\n",
    "# old styled direct access to tensors data attribute\n",
    "w2.data.normal_()\n",
    "\n",
    "# example for some module\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# for loop approach with direct access\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# scheduler example\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    train()\n",
    "    validate()\n",
    "\n",
    "# Train flag can be updated with boolean\n",
    "# to disable dropout and batch norm learning\n",
    "model.train(True)\n",
    "# execute train step\n",
    "model.train(False)\n",
    "# run inference step\n",
    "\n",
    "# CPU seed\n",
    "torch.manual_seed(42)\n",
    "# GPU seed\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.nn as nn\n",
    "save_path = 'test.mdl'\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "    ('relu2', nn.ReLU())\n",
    "]))\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Sequential (\n",
    "#   (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
    "#   (relu1): ReLU ()\n",
    "#   (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
    "#   (relu2): ReLU ()\n",
    "# )\n",
    "\n",
    "# save/load only the model parameters(prefered solution)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# save whole model\n",
    "torch.save(model, save_path)\n",
    "model = torch.load(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_gpu(x):\n",
    "    return x.cuda()\n",
    "\n",
    "class ImagesDataset(torch.utils.data.Dataset):\n",
    "    pass\n",
    "\n",
    "class Net(nn.Module):\n",
    "    pass\n",
    "\n",
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "fit_dataset = ImagesDataset(path_to_fit_images)\n",
    "fit_data_loader = torch.utils.data.DataLoader(fitdataset, batch_size=10)\n",
    "\n",
    "valid_dataset = ImagesDataset(path_to_valid_images)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=10)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    # training\n",
    "    lr_scheduler.step()\n",
    "    for inputs, labels in fit_data_loader:\n",
    "        inputs = Variable(to_gpu(inputs))\n",
    "        labels = Variable(to_gpu(labels))\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation:\n",
    "    # TODO: spell this out!\n",
    "    valid_inputs, valid_labels = get_data()\n",
    "    \n",
    "    inputs = Variable(to_gpu(valid_inputs))\n",
    "    labels = Variable(to_gpu(valid_labels))\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    if loss < best_valid_loss:\n",
    "        best_valid_loss = loss\n",
    "        # spell_out:\n",
    "        save_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "### tensor example\n",
    "x_cpu = torch.randn(10, 20)\n",
    "w_cpu = torch.randn(20, 10)\n",
    "# direct transfer to the GPU\n",
    "x_gpu = x_cpu.cuda()\n",
    "w_gpu = w_cpu.cuda()\n",
    "result_gpu = x_gpu @ w_gpu\n",
    "# get back from GPU to CPU\n",
    "result_cpu = result_gpu.cpu()\n",
    "print(result_cpu)\n",
    "\n",
    "### model example\n",
    "model = model.cuda()\n",
    "# train step\n",
    "inputs = Variable(inputs.cuda())\n",
    "outputs = model(inputs)\n",
    "# get back from GPU to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "x = Variable(torch.randn(10, 20), requires_grad=False)\n",
    "y = Variable(torch.randn(10, 3), requires_grad=False)\n",
    "# define some weights\n",
    "w1 = Variable(torch.randn(20, 5), requires_grad=True)\n",
    "w2 = Variable(torch.randn(5, 3), requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "for step in range(5):\n",
    "    pred = F.sigmoid(x @ w1)\n",
    "    pred = F.sigmoid(pred @ w2)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # manually zero all previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    # calculate new gradients\n",
    "    loss.backward()\n",
    "    # apply new gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(3, 4), requires_grad=False)\n",
    "y = Variable(torch.randn(3, 2), requires_grad=False)\n",
    "# define some weights\n",
    "w1 = Variable(torch.randn(4, 2), requires_grad=True)\n",
    "w2 = Variable(torch.FloatTensor(w1.data.numpy()), requires_grad=True)\n",
    "for i in range(5):\n",
    "    loss1 = torch.mean((y - x @ w1) ** 2)\n",
    "    loss2 = torch.mean((y - x @ w2) ** 2)\n",
    "# calculate the gradients\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "    print(\"w1 grad(zeroed)\", w1.grad)\n",
    "    print(\"w2 grad(not zeroed)\", w2.grad)\n",
    "    w1.grad.data.zero_()\n",
    "    print('-'*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
